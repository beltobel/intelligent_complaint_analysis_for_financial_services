{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5be506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install  langchain sentence-transformers chromadb nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b81f0efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "import os\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c8e9767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed dataset\n",
    "df = pd.read_csv('../data/processed/preprocessed_complaints.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cfa79a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns in the DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Complaint ID', 'Product', 'Sub-product', 'Cleaned_Narrative'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nColumns in the DataFrame:\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c92af23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 200 chunks into ChromaDB vector store at './vector_store'.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # Target chunk size in characters\n",
    "    chunk_overlap=50,  # Overlap to maintain context\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Initialize the embedding model\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Initialize ChromaDB client and create/get collection\n",
    "client = chromadb.PersistentClient(path=\"./vector_store\")\n",
    "collection = client.get_or_create_collection(name=\"cfpb_complaints\")\n",
    "\n",
    "# SAMPLE ONLY 100 ROWS TO SAVE MEMORY\n",
    "sample_df = df.sample(n=100, random_state=42)  # You can change n=100 as needed\n",
    "\n",
    "# Function to chunk text and generate metadata\n",
    "def process_narrative(row):\n",
    "    narrative = row['Cleaned_Narrative']\n",
    "    complaint_id = row['Complaint ID']\n",
    "    product = row['Product']\n",
    "    sub_product = row['Sub-product'] if pd.notnull(row['Sub-product']) else \"\"\n",
    "    \n",
    "    # Split the narrative into chunks\n",
    "    chunks = text_splitter.split_text(narrative)\n",
    "    \n",
    "    # Prepare data for vector store\n",
    "    documents = []\n",
    "    embeddings = []\n",
    "    metadatas = []\n",
    "    ids = []\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # Generate embedding for the chunk\n",
    "        embedding = embedding_model.encode(chunk).tolist()\n",
    "        \n",
    "        # Create unique ID for the chunk\n",
    "        chunk_id = f\"{complaint_id}_{i}\"\n",
    "        \n",
    "        # Store chunk, embedding, and metadata\n",
    "        documents.append(chunk)\n",
    "        embeddings.append(embedding)\n",
    "        metadatas.append({\n",
    "            \"Complaint ID\": complaint_id,\n",
    "            \"Product\": product,\n",
    "            \"Sub-product\": sub_product,\n",
    "            \"Chunk Index\": i\n",
    "        })\n",
    "        ids.append(chunk_id)\n",
    "    \n",
    "    return documents, embeddings, metadatas, ids\n",
    "\n",
    "# Process only the sample\n",
    "all_documents = []\n",
    "all_embeddings = []\n",
    "all_metadatas = []\n",
    "all_ids = []\n",
    "\n",
    "for _, row in sample_df.iterrows():\n",
    "    docs, embs, metas, ids = process_narrative(row)\n",
    "    all_documents.extend(docs)\n",
    "    all_embeddings.extend(embs)\n",
    "    all_metadatas.extend(metas)\n",
    "    all_ids.extend(ids)\n",
    "\n",
    "# Add to ChromaDB collection\n",
    "collection.add(\n",
    "    documents=all_documents,\n",
    "    embeddings=all_embeddings,\n",
    "    metadatas=all_metadatas,\n",
    "    ids=all_ids,\n",
    ")\n",
    "print(f\"Indexed {len(all_documents)} chunks into ChromaDB vector store at './vector_store'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deb1d32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
